{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb44338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ndalcin/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from jax import jit, vmap\n",
    "from functools import partial\n",
    "from params_init import create_conn_matrix\n",
    "from rk4_ode_solver import solve_ode\n",
    "from functions import cross_entropy_grad, cross_entropy_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b68d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (training set: 60,000 imgs | test set: 10,000 imgs)\n",
    "(input_train, label_train), (input_test, label_test) = tf.keras.datasets.mnist.load_data()\n",
    "# Create one-hot encoding\n",
    "num_classes = 10  # MNIST has 10 classes (digits 0-9)\n",
    "label_train_one_hot = jnp.eye(num_classes)[label_train]\n",
    "label_test_one_hot = jnp.eye(num_classes)[label_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4782efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network dynamics\n",
    "@jit\n",
    "def system(t, y, kappa, kappa1, g, J, x_in, p_in):\n",
    "    N = len(kappa)\n",
    "    x = y[:N]\n",
    "    p = y[N:]\n",
    "    dxdt = -0.5 * (kappa+kappa1) * x + 0.5 * g * (x**2 + p**2) * p + jnp.dot(J, p) - jnp.sqrt(kappa) * x_in\n",
    "    dpdt = -0.5 * (kappa+kappa1) * p - 0.5 * g * (x**2 + p**2) * x - jnp.dot(J, x) - jnp.sqrt(kappa) * p_in\n",
    "    return jnp.concatenate([dxdt, dpdt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b172dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params update according to Scattering Backpropagation \n",
    "@partial(jit, static_argnums=(5,))\n",
    "def update_weights(vec, x_free, p_free, graph, J, N):\n",
    "    def compute_dFdJ_jl(j, l):\n",
    "        dFdJ_jl = jnp.zeros(2 * N)  \n",
    "        dFdJ_jl = dFdJ_jl.at[j].set(p_free[l])\n",
    "        dFdJ_jl = dFdJ_jl.at[l].set(p_free[j])\n",
    "        dFdJ_jl = dFdJ_jl.at[j + N].set(-x_free[l])\n",
    "        dFdJ_jl = dFdJ_jl.at[l + N].set(-x_free[j])\n",
    "        \n",
    "        dFdJ_jl = dFdJ_jl.at[j].set(jnp.where(j == l, dFdJ_jl[j] / 2, dFdJ_jl[j]))\n",
    "        dFdJ_jl = dFdJ_jl.at[j + N].set(jnp.where(j == l, dFdJ_jl[j + N] / 2, dFdJ_jl[j + N]))\n",
    "    \n",
    "        return dFdJ_jl\n",
    "    \n",
    "    def body_fn(edge):\n",
    "        j, l = edge\n",
    "        dFdJ_jl = compute_dFdJ_jl(j, l)\n",
    "        dJ_jl = jnp.dot(dFdJ_jl, vec)\n",
    "        return dJ_jl\n",
    "    \n",
    "    updates = vmap(body_fn)(graph)\n",
    "\n",
    "    return updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "291cce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation_loop(key, J, input_pixels, lower, upper, tmax, num_steps, g):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test-set portion from 'lower' to 'upper'.\n",
    "    Args: \n",
    "    - key : random key\n",
    "    - J : parameters (connectivity matrix)\n",
    "    - input_pixels : total-number of input pixels (784 for MNIST)\n",
    "    - lower : lowest index to test in the training set (min is 0 for MNIST)\n",
    "    - upper : highest index to test in the training set (max is 10,000 for MNIST)\n",
    "    - tmax : final time for dynamic symulation\n",
    "    - num_steps : number of steps in RK4 (dt = tmax / num_steps)\n",
    "    - g : nonlinearity strength\n",
    "    Returns: cross-entropy-loss, accuracy rate\n",
    "    \"\"\"\n",
    "    N = J.shape[0]\n",
    "    y0 = jax.random.normal(key, shape=(2 * N,))\n",
    "    kappa = 1. * jnp.ones((N,))\n",
    "    kappa1 = 1. * jnp.ones((N,))\n",
    "    loss = 0\n",
    "    acc_rate = 0\n",
    "    dt = tmax / num_steps\n",
    "    \n",
    "    def evolution(idx, carry):\n",
    "        y0, J, loss, acc_rate = carry\n",
    "        # Use dynamic indexing in place of direct indexing\n",
    "        input_vec = jax.lax.dynamic_index_in_dim(input_test, idx, keepdims=False)\n",
    "        target = jax.lax.dynamic_index_in_dim(label_test_one_hot, idx, keepdims=False)\n",
    "\n",
    "        # Encode inputs\n",
    "        x_in = jnp.zeros(N)\n",
    "        p_in = jnp.zeros(N)\n",
    "        x_in = x_in.at[:input_pixels].add(jnp.reshape(input_vec, -1) / 100) # rescale input pixel in the (0, 2.55) interval\n",
    "\n",
    "        # Inference Phase\n",
    "        _ , out = solve_ode(system, y0, (0., tmax), num_steps, dt, kappa, kappa1, g, J, x_in, p_in)\n",
    "        solution_free = out[-1,:]\n",
    "\n",
    "        x_free = solution_free[:N]\n",
    "        y0 = solution_free # update initial condition\n",
    "\n",
    "        # Compute loss for the current sample\n",
    "        loss += cross_entropy_loss(x_free[-10:], target)\n",
    "        prediction = jnp.argmax(x_free[-10:])\n",
    "        acc_rate += (prediction == jnp.argmax(target))\n",
    "        return y0, J, loss, acc_rate\n",
    "    \n",
    "    y0, J, loss, acc_rate = jax.lax.fori_loop(lower, upper, evolution, (y0, J, loss, acc_rate) )\n",
    "    loss = loss / (upper - lower) \n",
    "    acc_rate /= (upper - lower)\n",
    "    \n",
    "    return loss, acc_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9763eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_loop(key, params_history, input_pixels, num_epochs,  batch_size,  beta, learning_rate, lower, upper, tmax, num_steps, g):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test-set portion from 'lower' to 'upper'.\n",
    "    Args: \n",
    "    - key : random key\n",
    "    - params_history : list of parameters after each training epoch\n",
    "    - input_pixels : total-number of input pixels (784 for MNIST)\n",
    "    - num_epochs : number of training epochs\n",
    "    - batch_size : used for averaging (approximate) gradients with SGD\n",
    "    - beta : perturbation strength (for the Feedback Phase)\n",
    "    - learning_rate : learning rate\n",
    "    - lower : lowest index to test in the training set (min is 0 for MNIST)\n",
    "    - upper : highest index to test in the training set (max is 60,000 // batchsize for MNIST)\n",
    "    - tmax : final time for dynamic symulation\n",
    "    - num_steps : number of steps in RK4 (dt = tmax / num_steps)\n",
    "    - g : nonlinearity strength\n",
    "    Returns: cross-entropy-loss, accuracy rate\n",
    "    \"\"\"\n",
    "    loss_history = [] \n",
    "    test_loss_history = []\n",
    "    acc_history = []\n",
    "    test_acc_history = []\n",
    "    J = params_history[0]\n",
    "    N = J.shape[0]\n",
    "    \n",
    "    kappa = 1. * jnp.ones((N,))\n",
    "    kappa1 = 1. * jnp.ones((N,))\n",
    "    \n",
    "    sigma_x = jnp.block([\n",
    "    [jnp.zeros((N,N)), jnp.eye(N)],\n",
    "    [jnp.eye(N), jnp.zeros((N,N))]\n",
    "    ])\n",
    "\n",
    "    # Take note of the upper-triangular nonzero entries' indexes\n",
    "    graph = jnp.stack(jnp.where(J != 0)).T\n",
    "    dt = tmax / num_steps\n",
    "    \n",
    "    batches = jnp.array([jnp.arange(idx * batch_size, (idx + 1) * batch_size) for idx in range(lower, upper)])\n",
    "\n",
    "    def evolution(idx, carry):\n",
    "        epoch_loss, epoch_acc, J = carry\n",
    "\n",
    "        def single_input_ev(j):\n",
    "            y0 = jax.random.normal(key, shape=(2 * N,))\n",
    "            # Use dynamic indexing in place of direct indexing\n",
    "            input_vec = jax.lax.dynamic_index_in_dim(input_train_shuffled, j, keepdims=False)\n",
    "            target = jax.lax.dynamic_index_in_dim(label_train_one_hot_shuffled, j, keepdims=False)\n",
    "\n",
    "            # Encode inputs\n",
    "            x_in = jnp.zeros(N)\n",
    "            p_in = jnp.zeros(N)\n",
    "            x_in = x_in.at[:input_pixels].add(jnp.reshape(input_vec, -1) / 100) # rescale input pixel in the (0, 2.55) interval\n",
    "\n",
    "            # Inference Phase\n",
    "            _ , out = solve_ode(system, y0, (0., tmax), num_steps, dt, kappa, kappa1, g, J, x_in, p_in)\n",
    "            solution_free = out[-1,:]\n",
    "\n",
    "            x_free = solution_free[:N]\n",
    "            p_free = solution_free[N:]\n",
    "            y0 = solution_free \n",
    "\n",
    "            # Feedback Phase\n",
    "            p_in = p_in.at[-10:].add(beta * cross_entropy_grad(x_free[-10:],target) ) # inject error signal\n",
    "            _ , out = solve_ode(system, y0, (0., tmax/2), num_steps//2, dt, kappa, kappa1, g, J, x_in, p_in)\n",
    "            solution_perturbed = out[-1,:]\n",
    "\n",
    "            # Update weights\n",
    "            # Compute right-part of SB's gradient approx formula \n",
    "            # (Note that U' = sigma_x corresponds to the quasi-symmetry U=sigma_y in the a(t)-basis)\n",
    "            vec = (learning_rate) * sigma_x @ (solution_perturbed - solution_free) / beta \n",
    "\n",
    "            return update_weights(vec, x_free, p_free, graph, J, N), cross_entropy_loss(x_free[-10:], target), accuracy(x_free, target)\n",
    "\n",
    "        updates, losses, accs = vmap(single_input_ev)(batches[idx]) \n",
    "\n",
    "        average_updates = jnp.mean(updates, axis = 0)\n",
    "\n",
    "        J = J.at[graph[:,0], graph[:,1]].add(average_updates)\n",
    "\n",
    "        epoch_loss += jnp.sum(losses) \n",
    "        epoch_acc += jnp.sum(accs)\n",
    "        \n",
    "        return epoch_loss, epoch_acc, J\n",
    "    \n",
    "    best_acc = 0\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs): \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        # Shuffle training set\n",
    "        key, subkey = jax.random.split(key)\n",
    "        indices = jax.random.permutation(subkey, upper*batch_size)\n",
    "        input_train_shuffled = input_train[indices]\n",
    "        label_train_one_hot_shuffled = label_train_one_hot[indices]\n",
    "        # Training\n",
    "        epoch_loss, epoch_acc, J = jax.lax.fori_loop(lower, upper, evolution, (epoch_loss, epoch_acc, J) )\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss = epoch_loss / ( (upper - lower) * batch_size )\n",
    "        epoch_acc = epoch_acc / ( (upper - lower) * batch_size )\n",
    "        loss_history.append(epoch_loss)\n",
    "        acc_history.append(epoch_acc)\n",
    "        # Learning rate update\n",
    "        ###\n",
    "        # Compute test accuracy\n",
    "        key, subkey = jax.random.split(key)       \n",
    "        epoch_test_loss, epoch_test_acc = Evaluation_loop(key = subkey, J = J, input_pixels = input_pixels, lower = 0, upper = 10000, tmax = tmax, num_steps = num_steps, g = g)\n",
    "\n",
    "        # Save the best weights\n",
    "        if epoch_test_acc > best_acc:\n",
    "            best_acc = epoch_test_acc\n",
    "            params_history[0] = J\n",
    "\n",
    "        test_loss_history.append(epoch_test_loss)\n",
    "        test_acc_history.append(epoch_test_acc)\n",
    "        print(f\"End of Epoch {epoch+1}: Train loss = {epoch_loss}, Train Accuracy = {epoch_acc}, Test loss = {epoch_test_loss},  Test Accuracy = {epoch_test_acc}, ||J||_2 = {jnp.linalg.norm(J)}\", flush=True)\n",
    "\n",
    "    return loss_history, acc_history, test_loss_history, test_acc_history, params_history, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(41) \n",
    "key, subkey1, subkey2 = jax.random.split(key, num=3)\n",
    "\n",
    "# Kernel dimension\n",
    "kernel_sizes = [6,4]\n",
    "strides = [2,2]\n",
    "\n",
    "J = create_conn_matrix(subkey1, kernel_sizes = [6,4], strides = [2,2], input_pixels = 28, num_logits = 10)\n",
    "#print(f\"||J||_2 = {jnp.linalg.norm(J)}\", flush=True)\n",
    "\n",
    "params_history = [J]\n",
    "\n",
    "input_pixels = 784\n",
    "num_epochs = 30\n",
    "batch_size = 10\n",
    "beta = 0.01\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Interval [lower, upper*batch_size] of images in the training set to use for the training\n",
    "lower = 0\n",
    "upper = 60000 // batch_size\n",
    "\n",
    "tmax = 60.\n",
    "num_steps = 600\n",
    "g = 0.2\n",
    "\n",
    "train_loss_history, train_acc_history, test_loss_history, test_acc_history, params_history, J = Training_loop(subkey2, \n",
    "                                                                                  params_history, \n",
    "                                                                                  input_pixels, \n",
    "                                                                                  num_epochs, \n",
    "                                                                                  batch_size, \n",
    "                                                                                  beta, \n",
    "                                                                                  learning_rate, \n",
    "                                                                                  lower, \n",
    "                                                                                  upper, \n",
    "                                                                                  tmax, \n",
    "                                                                                  num_steps, \n",
    "                                                                                  g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea893b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_acc_history, label='Train Accuracy')\n",
    "plt.plot(test_acc_history, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
